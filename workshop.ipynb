{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "260c0176-341b-4a25-899d-3ede85e01009",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Import Data\n",
    "In order to train our model, we are going to import time series data of room temperature.\n",
    "\n",
    "With our RNN, we are going to try to forecast the temperature of a room given a certain air flow temperature.\n",
    "\n",
    "You can find this dataset here: https://www.kaggle.com/datasets/vitthalmadane/ts-temp-1/data?select=MLTempDataset.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7df5fc72-8fc1-4b6a-b33c-c2934f5250f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/noahcampise/.cache/kagglehub/datasets/vitthalmadane/ts-temp-1/versions/2\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version of dataset to local filesystem\n",
    "path = kagglehub.dataset_download(\"vitthalmadane/ts-temp-1\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395ffddd-d9d5-407b-8d68-0663c5b57c00",
   "metadata": {},
   "source": [
    "## Read Data into Memory\n",
    "For this workshop, we are going to use pandas to read the data into memory and convert it into a numpy array to train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20031837-ed90-45f1-92c0-c5a99a4882dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "file_path = os.path.join(path, 'MLTempDataset.csv')\n",
    "temperature_data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f70790c-33f2-493a-ae8a-cd4c30e3f317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.DataFrame'>\n",
      "RangeIndex: 6676 entries, 0 to 6675\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Unnamed: 0  6676 non-null   int64  \n",
      " 1   Datetime1   6676 non-null   int64  \n",
      " 2   DAYTON_MW   6676 non-null   float64\n",
      " 3   Datetime    6676 non-null   str    \n",
      "dtypes: float64(1), int64(2), str(1)\n",
      "memory usage: 208.8 KB\n"
     ]
    }
   ],
   "source": [
    "temperature_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd30511c-884c-4348-97bb-c98614813239",
   "metadata": {},
   "source": [
    "## Data Visualization\n",
    "\n",
    "In order to train our model, we need to understand what we are training the model on.\n",
    "\n",
    "If we look at the top 5 rows of our temperature data, we are given 4 columns:\n",
    "\n",
    "1. Unnamed: This represents the index of the data, since our data is in order, we can ignore this.\n",
    "2. Datetime1: This is the hour of the day between 0 and 23\n",
    "3. DAYTON_MW: This is the temperature of the room in Celsius\n",
    "4. Datetime: This is the Date and hour of the day in a Datetime Format\n",
    "\n",
    "We want our model to forecast temperature over time. So, we will treat our temperature as our label and we will Datetime as the time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87e63692-bfe5-4700-93dd-7b46bbf9a464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Datetime1</th>\n",
       "      <th>DAYTON_MW</th>\n",
       "      <th>Datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20.867</td>\n",
       "      <td>2022-01-04 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>21.000</td>\n",
       "      <td>2022-01-04 01:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>20.867</td>\n",
       "      <td>2022-01-04 02:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>20.650</td>\n",
       "      <td>2022-01-04 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>20.400</td>\n",
       "      <td>2022-01-04 04:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Datetime1  DAYTON_MW             Datetime\n",
       "0           0          0     20.867  2022-01-04 00:00:00\n",
       "1           1          1     21.000  2022-01-04 01:00:00\n",
       "2           2          2     20.867  2022-01-04 02:00:00\n",
       "3           3          3     20.650  2022-01-04 03:00:00\n",
       "4           4          4     20.400  2022-01-04 04:00:00"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperature_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d183b6d-eae2-42de-b478-5886d4a1c1f4",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "Continuing with the data aspect of machine learning, we need to preprocess our data in a way that facilitates training for time series data.\n",
    "\n",
    "As part of this, we will need to standardize our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3328713e-48a1-4c1f-8405-f8f2005ce645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 20.867, '2022-01-04 00:00:00'],\n",
       "       [1, 1, 21.0, '2022-01-04 01:00:00'],\n",
       "       [2, 2, 20.867, '2022-01-04 02:00:00'],\n",
       "       ...,\n",
       "       [6673, 21, 26.45, '2022-10-09 01:00:00'],\n",
       "       [6674, 22, 25.9, '2022-10-09 02:00:00'],\n",
       "       [6675, 23, 25.567, '2022-10-09 03:00:00']],\n",
       "      shape=(6676, 4), dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert pandas data to numpy for training\n",
    "temperature_data = temperature_data.to_numpy()\n",
    "temperature_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb66eb47-04bd-4c04-8576-098d74c30e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to create dataset\n",
    "# Basically, we are saying, take 60 sequential temperatures, then, given this information\n",
    "# predict what the next temperature would be.\n",
    "def create_dataset(data, time_step=60):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_step - 1):\n",
    "        X.append(data[i:(i + time_step), 2])\n",
    "        y.append(data[i + time_step, 2])\n",
    "    return np.array(X).astype('float64'), np.array(y).astype('float64')\n",
    "\n",
    "# X, y = create_dataset(temperature_data)\n",
    "# X = X.reshape(X.shape[0], X.shape[1], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c703cfb1-69d9-4f71-a9f2-3e13bcb3a536",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = create_dataset(temperature_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "644964fa-65bb-4365-bd69-100b28528733",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(X.shape[0], X.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d10e920-3994-40a1-9d1f-4a8e8b0e2d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018a76e1-399a-4f57-95ba-878ddc39e3b8",
   "metadata": {},
   "source": [
    "# Build the Model\n",
    "\n",
    "Using Tensorflow and Keras, we are going to build a Recurrent Nueral Network to forecast room temperature. In particular, we are going to use a Gated Recurrent Unit Model. \n",
    "\n",
    "At a high level, this addresses the vanishing gradient problem, allowing them to reliably predict over longer periods of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "707299c1-fa5d-47bf-b24e-44ce028e7bf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                       │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ gru (\u001b[38;5;33mGRU\u001b[0m)                       │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, GRU\n",
    "model = Sequential([\n",
    "    # GRU(64, activation='tanh', return_sequences=True, input_shape=(10, 5)),  # First GRU layer\n",
    "    GRU(64, activation='tanh'),  # Second GRU layer\n",
    "    Dense(1)  # Output layer for binary classification\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['r2_score'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb73fbc-fbbe-4fcd-a2d0-5c8fd5023746",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "Now, we are going to train our model to fit our data and hopefully get a descent temperature forecaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30724025-7dea-48cf-9634-015792a42810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 306.6031 - r2_score: -4.8947\n",
      "Epoch 2/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 114.4958 - r2_score: -1.2013\n",
      "Epoch 3/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 67.6649 - r2_score: -0.3009\n",
      "Epoch 4/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 39.2678 - r2_score: 0.2450\n",
      "Epoch 5/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 25.3228 - r2_score: 0.5131\n",
      "Epoch 6/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 18.3248 - r2_score: 0.6477\n",
      "Epoch 7/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 14.5871 - r2_score: 0.7195\n",
      "Epoch 8/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 12.4775 - r2_score: 0.7601\n",
      "Epoch 9/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 11.2716 - r2_score: 0.7833\n",
      "Epoch 10/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 10.4332 - r2_score: 0.7994\n",
      "Epoch 11/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 9.8843 - r2_score: 0.8100\n",
      "Epoch 12/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 9.5107 - r2_score: 0.8171\n",
      "Epoch 13/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 9.2152 - r2_score: 0.8228\n",
      "Epoch 14/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 8.9801 - r2_score: 0.8273\n",
      "Epoch 15/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 8.8129 - r2_score: 0.8306\n",
      "Epoch 16/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 8.6767 - r2_score: 0.8332\n",
      "Epoch 17/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 8.5971 - r2_score: 0.8347\n",
      "Epoch 18/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 8.4660 - r2_score: 0.8372\n",
      "Epoch 19/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 8.3801 - r2_score: 0.8389\n",
      "Epoch 20/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 8.3004 - r2_score: 0.8404\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=20, batch_size=64)\n",
    "\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14612461-7077-45ce-ae54-2e266156cb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 - 0s - 5ms/step - loss: 2.5516 - r2_score: 0.8794\n",
      "\n",
      "Test accuracy: 0.87938392162323\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test,  y_test, verbose=2)\n",
    "\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0926944b-7696-4359-bb99-4d41dcee61a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[23.87537 ],\n",
       "       [25.065208],\n",
       "       [25.323307],\n",
       "       [25.668158],\n",
       "       [25.916805],\n",
       "       [11.763302],\n",
       "       [10.515472],\n",
       "       [11.079081],\n",
       "       [17.061882],\n",
       "       [12.486538]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_train, verbose=0)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58f5ddb3-8d68-475f-951b-f9da2928ad21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24.867, 25.5  , 26.   , 26.267,  9.633,  9.267,  8.85 , 15.6  ,\n",
       "       12.267,  9.067])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:10].transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fdc5f4-15d6-4e3d-bd3f-ecd598e42550",
   "metadata": {},
   "source": [
    "## Exporting the model\n",
    "At this point, we have a satisfactory model. At this point, we are going to export this model to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed2944b8-6faa-41c4-901a-9090e1b79ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: models: File exists\n",
      "INFO:tensorflow:Assets written to: models/my_model.pb/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/my_model.pb/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'models/my_model.pb'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 60, 1), dtype=tf.float32, name='keras_tensor')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  4722221008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  4722219088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  4722219472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  4722218704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  4722221776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    }
   ],
   "source": [
    "!mkdir models\n",
    "model.export('models/my_model.pb')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72475e1e-f959-4404-b9fe-01237b2aa71d",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "There are techniques that we can use to reduce the size of Nueral Networks to reduce their size\n",
    "and their computational cost.\n",
    "\n",
    "Some of these include:\n",
    "1. Weight Pruning\n",
    "2. Weight Clustering\n",
    "3. Quantization\n",
    "\n",
    "For this workshop, we will talk about all three, but only implement quantization for our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1faefbb8-a4d1-45af-a15b-1009f5ddbcc1",
   "metadata": {},
   "source": [
    "# Weight Pruning\n",
    "\n",
    "Magnitude-based weight pruning gradually zeroes out model weights during the training process to achieve model sparsity. Sparse models are easier to compress, and we can skip the zeroes during inference for latency improvements.\n",
    "\n",
    "# Weight Clustering\n",
    "\n",
    "Magnitude-based weight pruning gradually zeroes out model weights during the training process to achieve model sparsity. Sparse models are easier to compress, and we can skip the zeroes during inference for latency improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0922046d-0bd5-42a8-9b8a-2997851516be",
   "metadata": {},
   "source": [
    "## Quantization\n",
    "Quantization aware training emulates inference-time quantization, creating a model that downstream tools will use to produce actually quantized models. The quantized models use lower-precision (e.g. 8-bit instead of 32-bit float), leading to benefits during deployment.\n",
    "\n",
    "### Deploy with quantization\n",
    "\n",
    "Quantization brings improvements via model compression and latency reduction. With the API defaults, the model size shrinks by 4x, and we typically see between 1.5 - 4x improvements in CPU latency in the tested backends. Eventually, latency improvements can be seen on compatible machine learning accelerators, such as the EdgeTPU and NNAPI.\n",
    "\n",
    "In our workshop, we are going to use Post-training quantization, which will allow us to reduce model size with our already trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f1628683-3319-4087-9e69-8e5801735691",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1769574384.694122  175412 tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "W0000 00:00:1769574384.694148  175412 tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2026-01-27 23:26:24.694347: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: models/my_model.pb\n",
      "2026-01-27 23:26:24.694694: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2026-01-27 23:26:24.694698: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: models/my_model.pb\n",
      "2026-01-27 23:26:24.697722: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2026-01-27 23:26:24.709637: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: models/my_model.pb\n",
      "2026-01-27 23:26:24.715008: I tensorflow/cc/saved_model/loader.cc:471] SavedModel load for tags { serve }; Status: success: OK. Took 20666 microseconds.\n",
      "2026-01-27 23:26:24.756750: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:4082] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s):\n",
      "Flex ops: FlexTensorListReserve, FlexTensorListSetItem, FlexTensorListStack\n",
      "Details:\n",
      "\ttf.TensorListReserve(tensor<2xi32>, tensor<i32>) -> (tensor<!tf_type.variant<tensor<?x64xf32>>>) : {device = \"\"}\n",
      "\ttf.TensorListSetItem(tensor<!tf_type.variant<tensor<?x64xf32>>>, tensor<i32>, tensor<?x64xf32>) -> (tensor<!tf_type.variant<tensor<?x64xf32>>>) : {device = \"\", resize_if_index_out_of_bounds = false}\n",
      "\ttf.TensorListStack(tensor<!tf_type.variant<tensor<?x64xf32>>>, tensor<2xi32>) -> (tensor<1x?x64xf32>) : {device = \"\", num_elements = 1 : i64}\n",
      "See instructions: https://www.tensorflow.org/lite/guide/ops_select\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "28824"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# Here is how to specify 8 bit integer weight quantization\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model('models/my_model.pb')\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "converter._experimental_lower_tensor_list_ops = False\n",
    "tflite_quant_model = converter.convert()\n",
    "open(\"models/converted_model.tflite\", \"wb\").write(tflite_quant_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4d07645b-7f82-4f40-a3bb-18492b3cc537",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TFLite ModelAnalyzer ===\n",
      "\n",
      "Your TFLite model has '3' subgraph(s). In the subgraph description below,\n",
      "T# represents the Tensor numbers. For example, in Subgraph#0, the SHAPE op takes\n",
      "tensor #0 as input and produces tensor #15 as output.\n",
      "\n",
      "Subgraph#0 main(T#0) -> [T#28]\n",
      "  Op#0 SHAPE(T#0) -> [T#15]\n",
      "  Op#1 FlexTensorListReserve(T#13[-1, 64], T#12[1]) -> [T#16]\n",
      "  Op#2 STRIDED_SLICE(T#15, T#11[0], T#10[1], T#10[1]) -> [T#17]\n",
      "  Op#3 TRANSPOSE(T#0, T#9[1, 0, 2]) -> [T#18]\n",
      "  Op#4 PACK(T#17, T#7[64]) -> [T#19]\n",
      "  Op#5 FILL(T#19, T#8) -> [T#20]\n",
      "  Op#6 WHILE(T#14[0], T#14[0], T#16, T#20, T#18, Cond: Subgraph#1, Body: Subgraph#2) -> [T#21, T#22, T#23, T#24, T#25]\n",
      "  Op#7 FlexTensorListStack(T#23, T#13[-1, 64]) -> [T#26]\n",
      "  Op#8 STRIDED_SLICE(T#26, T#4[-1, 0, 0], T#3[0, 0, 64], T#2[1, 1, 1]) -> [T#27]\n",
      "  Op#9 FULLY_CONNECTED(T#27, T#5, T#1) -> [T#28]\n",
      "\n",
      "Tensors of Subgraph#0\n",
      "  T#0(serving_default_keras_tensor:0) shape_signature:[-1, 60, 1], type:FLOAT32\n",
      "  T#1(arith.constant) shape:[1], type:FLOAT32 RO 4 bytes, buffer: 2, data:[0.316925]\n",
      "  T#2(arith.constant1) shape:[3], type:INT32 RO 12 bytes, buffer: 3, data:[1, 1, 1]\n",
      "  T#3(arith.constant2) shape:[3], type:INT32 RO 12 bytes, buffer: 4, data:[0, 0, 64]\n",
      "  T#4(arith.constant3) shape:[3], type:INT32 RO 12 bytes, buffer: 5, data:[-1, 0, 0]\n",
      "  T#5(arith.constant4) shape:[1, 64], type:FLOAT32 RO 256 bytes, buffer: 6, data:[-0.36769, -0.58704, 0.284457, -0.579279, -0.395205, ...]\n",
      "  T#6(arith.constant5) shape:[], type:INT32 RO 4 bytes, buffer: 7, data:[60]\n",
      "  T#7(arith.constant6) shape:[], type:INT32 RO 4 bytes, buffer: 8, data:[64]\n",
      "  T#8(arith.constant7) shape:[], type:FLOAT32 RO 4 bytes, buffer: 9, data:[0]\n",
      "  T#9(arith.constant8) shape:[3], type:INT32 RO 12 bytes, buffer: 10, data:[1, 0, 2]\n",
      "  T#10(arith.constant9) shape:[1], type:INT32 RO 4 bytes, buffer: 11, data:[1]\n",
      "  T#11(arith.constant10) shape:[1], type:INT32 RO 4 bytes, buffer: 9, data:[0]\n",
      "  T#12(arith.constant11) shape:[], type:INT32 RO 4 bytes, buffer: 11, data:[1]\n",
      "  T#13(arith.constant12) shape:[2], type:INT32 RO 8 bytes, buffer: 14, data:[-1, 64]\n",
      "  T#14(arith.constant13) shape:[], type:INT32 RO 4 bytes, buffer: 9, data:[0]\n",
      "  T#15(sequential_1/gru_1/Shape) shape:[3], type:INT32\n",
      "  T#16(sequential_1/gru_1/TensorArrayV2_1) shape:[], type:VARIANT\n",
      "  T#17(sequential_1/gru_1/strided_slice) shape:[], type:INT32\n",
      "  T#18(sequential_1/gru_1/transpose) shape_signature:[60, -1, 1], type:FLOAT32\n",
      "  T#19(sequential_1/gru_1/zeros/packed) shape:[2], type:INT32\n",
      "  T#20(sequential_1/gru_1/zeros) shape_signature:[-1, 64], type:FLOAT32\n",
      "  T#21(sequential_1/gru_1/while) shape:[], type:INT32\n",
      "  T#22(sequential_1/gru_1/while1) shape:[], type:INT32\n",
      "  T#23(sequential_1/gru_1/while2) shape:[], type:VARIANT\n",
      "  T#24(sequential_1/gru_1/while3) shape_signature:[-1, 64], type:FLOAT32\n",
      "  T#25(sequential_1/gru_1/while4) shape_signature:[60, -1, 1], type:FLOAT32\n",
      "  T#26(sequential_1/gru_1/TensorArrayV2Stack/TensorListStack) shape_signature:[1, -1, 64], type:FLOAT32\n",
      "  T#27(sequential_1/gru_1/strided_slice_3) shape_signature:[-1, 64], type:FLOAT32\n",
      "  T#28(StatefulPartitionedCall_1:0) shape_signature:[-1, 1], type:FLOAT32\n",
      "\n",
      "Subgraph#1 sequential_1/gru_1/while_cond(T#1_0, T#1_1, T#1_2, T#1_3, T#1_4) -> [T#1_8]\n",
      "  Op#0 LESS(T#1_1, T#1_5[60]) -> [T#1_6]\n",
      "  Op#1 LESS(T#1_0, T#1_5[60]) -> [T#1_7]\n",
      "  Op#2 LOGICAL_AND(T#1_7, T#1_6) -> [T#1_8]\n",
      "\n",
      "Tensors of Subgraph#1\n",
      "  T#1_0(sequential_1/gru_1/while_cond_arg0) shape:[], type:INT32\n",
      "  T#1_1(sequential_1/gru_1/while_cond_arg1) shape:[], type:INT32\n",
      "  T#1_2(sequential_1/gru_1/while_cond_arg2) shape:[], type:VARIANT\n",
      "  T#1_3(sequential_1/gru_1/while_cond_arg3) shape_signature:[-1, 64], type:FLOAT32\n",
      "  T#1_4(sequential_1/gru_1/while_cond_arg4) shape_signature:[60, -1, 1], type:FLOAT32\n",
      "  T#1_5(arith.constant14) shape:[], type:INT32 RO 4 bytes, buffer: 7, data:[60]\n",
      "  T#1_6(sequential_1/gru_1/while/Less) shape:[], type:BOOL\n",
      "  T#1_7(sequential_1/gru_1/while/Less_1) shape:[], type:BOOL\n",
      "  T#1_8(sequential_1/gru_1/while/LogicalAnd) shape:[], type:BOOL\n",
      "\n",
      "Subgraph#2 sequential_1/gru_1/while_body(T#2_0, T#2_1, T#2_2, T#2_3, T#2_4) -> [T#2_22, T#2_17, T#2_39, T#2_38, T#2_4]\n",
      "  Op#0 ADD(T#2_1, T#2_6[1]) -> [T#2_17]\n",
      "  Op#1 FULLY_CONNECTED(T#2_3, T#2_16, T#2_14) -> [T#2_18]\n",
      "  Op#2 STRIDED_SLICE(T#2_18, T#2_11[0, 0], T#2_10[0, 64], T#2_9[1, 1]) -> [T#2_19]\n",
      "  Op#3 STRIDED_SLICE(T#2_18, T#2_10[0, 64], T#2_8[0, 128], T#2_9[1, 1]) -> [T#2_20]\n",
      "  Op#4 STRIDED_SLICE(T#2_18, T#2_8[0, 128], T#2_11[0, 0], T#2_9[1, 1]) -> [T#2_21]\n",
      "  Op#5 ADD(T#2_0, T#2_6[1]) -> [T#2_22]\n",
      "  Op#6 GATHER(T#2_4, T#2_1) -> [T#2_23]\n",
      "  Op#7 FULLY_CONNECTED(T#2_23, T#2_15, T#2_13) -> [T#2_24]\n",
      "  Op#8 SPLIT(T#2_12[-1], T#2_24) -> [T#2_25, T#2_26, T#2_27]\n",
      "  Op#9 ADD(T#2_25, T#2_19) -> [T#2_28]\n",
      "  Op#10 LOGISTIC(T#2_28) -> [T#2_29]\n",
      "  Op#11 MUL(T#2_29, T#2_3) -> [T#2_30]\n",
      "  Op#12 SUB(T#2_7, T#2_29) -> [T#2_31]\n",
      "  Op#13 ADD(T#2_26, T#2_20) -> [T#2_32]\n",
      "  Op#14 LOGISTIC(T#2_32) -> [T#2_33]\n",
      "  Op#15 MUL(T#2_33, T#2_21) -> [T#2_34]\n",
      "  Op#16 ADD(T#2_27, T#2_34) -> [T#2_35]\n",
      "  Op#17 TANH(T#2_35) -> [T#2_36]\n",
      "  Op#18 MUL(T#2_31, T#2_36) -> [T#2_37]\n",
      "  Op#19 ADD(T#2_30, T#2_37) -> [T#2_38]\n",
      "  Op#20 FlexTensorListSetItem(T#2_2, T#2_5[0], T#2_38) -> [T#2_39]\n",
      "\n",
      "Tensors of Subgraph#2\n",
      "  T#2_0(sequential_1/gru_1/while_body_arg0) shape:[], type:INT32\n",
      "  T#2_1(sequential_1/gru_1/while_body_arg1) shape:[], type:INT32\n",
      "  T#2_2(sequential_1/gru_1/while_body_arg2) shape:[], type:VARIANT\n",
      "  T#2_3(sequential_1/gru_1/while_body_arg3) shape_signature:[-1, 64], type:FLOAT32\n",
      "  T#2_4(sequential_1/gru_1/while_body_arg4) shape_signature:[60, -1, 1], type:FLOAT32\n",
      "  T#2_5(arith.constant15) shape:[], type:INT32 RO 4 bytes, buffer: 9, data:[0]\n",
      "  T#2_6(arith.constant16) shape:[], type:INT32 RO 4 bytes, buffer: 11, data:[1]\n",
      "  T#2_7(arith.constant17) shape:[], type:FLOAT32 RO 4 bytes, buffer: 46, data:[1]\n",
      "  T#2_8(arith.constant18) shape:[2], type:INT32 RO 8 bytes, buffer: 47, data:[0, 128]\n",
      "  T#2_9(arith.constant19) shape:[2], type:INT32 RO 8 bytes, buffer: 48, data:[1, 1]\n",
      "  T#2_10(arith.constant20) shape:[2], type:INT32 RO 8 bytes, buffer: 49, data:[0, 64]\n",
      "  T#2_11(arith.constant21) shape:[2], type:INT32 RO 8 bytes, buffer: 50, data:[0, 0]\n",
      "  T#2_12(arith.constant22) shape:[], type:INT32 RO 4 bytes, buffer: 51, data:[-1]\n",
      "  T#2_13(arith.constant23) shape:[192], type:FLOAT32 RO 768 bytes, buffer: 52, data:[-0.0824728, -0.0366655, -0.131108, -0.0474845, -0.0378365, ...]\n",
      "  T#2_14(arith.constant24) shape:[192], type:FLOAT32 RO 768 bytes, buffer: 53, data:[-0.0824728, -0.0366655, -0.131107, -0.0474845, -0.0378364, ...]\n",
      "  T#2_15(arith.constant25) shape:[192, 1], type:FLOAT32 RO 768 bytes, buffer: 54, data:[-0.145643, 0.0903475, -0.209632, 0.121376, 0.0507778, ...]\n",
      "  T#2_16(tfl.pseudo_qconst) shape:[192, 64], type:INT8 RO 12288 bytes, buffer: 55, data:[C, #, ., ., /, ...]\n",
      "  T#2_17(sequential_1/gru_1/while/add) shape:[], type:INT32\n",
      "  T#2_18(sequential_1/gru_1/while/gru_cell_1/MatMul_1;sequential_1/gru_1/while/gru_cell_1/add) shape_signature:[-1, 192], type:FLOAT32\n",
      "  T#2_19(sequential_1/gru_1/while/gru_cell_1/strided_slice) shape_signature:[-1, 64], type:FLOAT32\n",
      "  T#2_20(sequential_1/gru_1/while/gru_cell_1/strided_slice_1) shape_signature:[-1, 64], type:FLOAT32\n",
      "  T#2_21(sequential_1/gru_1/while/gru_cell_1/strided_slice_2) shape_signature:[-1, 64], type:FLOAT32\n",
      "  T#2_22(sequential_1/gru_1/while/add_1) shape:[], type:INT32\n",
      "  T#2_23(sequential_1/gru_1/while/TensorArrayV2Read/TensorListGetItem;) shape_signature:[-1, 1], type:FLOAT32\n",
      "  T#2_24(sequential_1/gru_1/while/gru_cell_1/MatMul;sequential_1/gru_1/while/gru_cell_1/BiasAdd) shape_signature:[-1, 192], type:FLOAT32\n",
      "  T#2_25(sequential_1/gru_1/while/gru_cell_1/split_1) shape_signature:[-1, 64], type:FLOAT32\n",
      "  T#2_26(sequential_1/gru_1/while/gru_cell_1/split_11) shape_signature:[-1, 64], type:FLOAT32\n",
      "  T#2_27(sequential_1/gru_1/while/gru_cell_1/split_12) shape_signature:[-1, 64], type:FLOAT32\n",
      "  T#2_28(sequential_1/gru_1/while/gru_cell_1/add_1) shape_signature:[-1, 64], type:FLOAT32\n",
      "  T#2_29(sequential_1/gru_1/while/gru_cell_1/Sigmoid) shape_signature:[-1, 64], type:FLOAT32\n",
      "  T#2_30(sequential_1/gru_1/while/gru_cell_1/mul_1) shape_signature:[-1, 64], type:FLOAT32\n",
      "  T#2_31(sequential_1/gru_1/while/gru_cell_1/sub) shape_signature:[-1, 64], type:FLOAT32\n",
      "  T#2_32(sequential_1/gru_1/while/gru_cell_1/add_2) shape_signature:[-1, 64], type:FLOAT32\n",
      "  T#2_33(sequential_1/gru_1/while/gru_cell_1/Sigmoid_1) shape_signature:[-1, 64], type:FLOAT32\n",
      "  T#2_34(sequential_1/gru_1/while/gru_cell_1/mul) shape_signature:[-1, 64], type:FLOAT32\n",
      "  T#2_35(sequential_1/gru_1/while/gru_cell_1/add_3) shape_signature:[-1, 64], type:FLOAT32\n",
      "  T#2_36(sequential_1/gru_1/while/gru_cell_1/Tanh) shape_signature:[-1, 64], type:FLOAT32\n",
      "  T#2_37(sequential_1/gru_1/while/gru_cell_1/mul_2) shape_signature:[-1, 64], type:FLOAT32\n",
      "  T#2_38(sequential_1/gru_1/while/gru_cell_1/add_4) shape_signature:[-1, 64], type:FLOAT32\n",
      "  T#2_39(sequential_1/gru_1/while/TensorArrayV2Write/TensorListSetItem) shape:[], type:VARIANT\n",
      "\n",
      "---------------------------------------------------------------\n",
      "Your TFLite model has '1' signature_def(s).\n",
      "\n",
      "Signature#0 key: 'serving_default'\n",
      "- Subgraph: Subgraph#0\n",
      "- Inputs: \n",
      "    'keras_tensor' : T#0\n",
      "- Outputs: \n",
      "    'output_0' : T#28\n",
      "\n",
      "---------------------------------------------------------------\n",
      "              Model size:      28824 bytes\n",
      "    Non-data buffer size:      13740 bytes (47.67 %)\n",
      "  Total data buffer size:      15084 bytes (52.33 %)\n",
      "          - Subgraph#0  :        344 bytes (01.19 %)\n",
      "          - Subgraph#1  :          4 bytes (00.01 %)\n",
      "          - Subgraph#2  :      14640 bytes (50.79 %)\n",
      "    (Zero value buffers):         12 bytes (00.04 %)\n",
      "\n",
      "* Buffers of TFLite model are mostly used for constant tensors.\n",
      "  And zero value buffers are buffers filled with zeros.\n",
      "  Non-data buffers area are used to store operators, subgraphs and etc.\n",
      "  You can find more details from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/schema/schema.fbs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.lite.experimental.Analyzer.analyze(model_content=tflite_quant_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "252ad4dc-4789-490f-b8d5-ca194d9c81dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/noahcampise/school-spring-2026/keras-workshop/.venv/lib/python3.12/site-packages/tensorflow/lite/python/interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
      "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
      "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
      "    for details.\n",
      "    \n",
      "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
     ]
    }
   ],
   "source": [
    "tflite_interpreter = tf.lite.Interpreter(model_path=\"models/converted_model.tflite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "995a8c83-3f41-413e-be1e-7fffa820f8e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'serving_default_keras_tensor:0',\n",
       "  'index': 0,\n",
       "  'shape': array([ 1, 60,  1], dtype=int32),\n",
       "  'shape_signature': array([-1, 60,  1], dtype=int32),\n",
       "  'dtype': numpy.float32,\n",
       "  'quantization': (0.0, 0),\n",
       "  'quantization_parameters': {'scales': array([], dtype=float32),\n",
       "   'zero_points': array([], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflite_interpreter.get_input_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1b00952e-16b7-4ddd-a719-505a382d407a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'StatefulPartitionedCall_1:0',\n",
       "  'index': 28,\n",
       "  'shape': array([1, 1], dtype=int32),\n",
       "  'shape_signature': array([-1,  1], dtype=int32),\n",
       "  'dtype': numpy.float32,\n",
       "  'quantization': (0.0, 0),\n",
       "  'quantization_parameters': {'scales': array([], dtype=float32),\n",
       "   'zero_points': array([], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflite_interpreter.get_output_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6b3d52c-3fbb-4f12-bf5d-95b694aef8c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tflite_interpreter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m input_index = \u001b[43mtflite_interpreter\u001b[49m.get_input_details()[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mindex\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      2\u001b[39m output_index = tflite_interpreter.get_output_details()[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mindex\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      4\u001b[39m test_temperatures = np.expand_dims(X_train[\u001b[32m0\u001b[39m], axis=\u001b[32m0\u001b[39m).astype(np.float32)\n",
      "\u001b[31mNameError\u001b[39m: name 'tflite_interpreter' is not defined"
     ]
    }
   ],
   "source": [
    "# I was not able to test the model. Weird version requirements.\n",
    "input_index = tflite_interpreter.get_input_details()[0][\"index\"]\n",
    "output_index = tflite_interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "test_temperatures = np.expand_dims(X_train[0], axis=0).astype(np.float32)\n",
    "\n",
    "tflite_interpreter.allocate_tensors()\n",
    "\n",
    "tflite_interpreter.set_tensor(input_index, test_temperatures)\n",
    "tflite_interpreter.invoke()\n",
    "predictions = tflite_interpreter.get_tensor(output_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce31c68-9b38-4fa6-884a-e29e5796b822",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
